---
title: "Replication of Twitter hate speech classification"
author: "Aboli Moroney, Mayank Goel, Samarth Modi, Harini Ramprasad"
knit: "bookdown::render_book"
output: word_document
bibliography: bibliography.bib
# link-citations: yes
# colorlinks: yes
# lot: yes
# lof: yes
# fontsize: 12pt
# monofont: "Source Code Pro"
# monofontoptions: "Scale=0.7"
# site: bookdown::bookdown_site
# #description: "A guide to authoring books with R Markdown, including how to generate figures and tables, and insert "
# url: 'https\://bookdown.org/yihui/bookdown/'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::read_chunk('001-tweet-cleaning.R')
```

## Introduction

Lately, there has been a lot of effort and research on identifying content that is abusive or offensive on online and social media. Twitter recently published a relatively large and reliable dataset on ‘Hate and Abusive Speech on Twitter’. As Data Scientists, we fully understand the need to find the best methods and data for identifying such content and flagging it as inappropriate.
 
In this project, our aim is to reproduce some of the findings in a research paper titled '[Comparative Studies of Detecting Abusive Language on Twitter](https://arxiv.org/abs/1808.10245)' [see @originalarticle] that performs a comparative study and provides suggestions for using additional features and data for improving such classification of hate and abusive speech using Twitter data. 

Using the data and code provided by the authors, we aim to replicate the efficacy and accuracy of machine learning models such as Naive Bayes, Logistic Regression, SVM and Random Forest presented in this paper. Our ultimate goal is to understand how each of these techniques performs on the different types of classifications and generate identical results for precision, recall and F1 score as reported by the authors. The choice of models to reproduce is based on the observation that the top precision has been generated by some of these models as well as our experimental constraints for executing the code.


## Data import and exploration

Importing the raw csv file. The csv file has about 30 columns. However, we would only need the following columns for our analyis.

1. tweet_id - unique identifier for each tweet 
2. tweet_text - raw tweet (uncleaned)
3. does_this_tweet_contain_hate_speech - (label with 3 different classes)


```{r raw-data-analysis, echo=FALSE, include=TRUE}
#Keeping only the necessary columns
library(here)
library(knitr)
raw_data <- read.csv(here("Data","twitter-hate-speech-classifier-data.csv"))
raw_data = raw_data[c("tweet_id", "tweet_text", "does_this_tweet_contain_hate_speech")]
print(c("total number of tweets", dim(raw_data)[1]))
kable(head(raw_data,10),caption = "raw data")
```


Adding Word Cloud of tweets 
```{r}
knitr::read_chunk('003-display-wordcloud.R')

```



```{r}
include_graphics(here("analysis","WordCloud.png"))
```




## Tweet classification

Our output label columns consists of 3 different classes as following:

1. The tweet contains hate speech 
2. The tweet is not offensive
3. The tweet uses offensive language but not hate speech	

```{r}
data.frame(summary(raw_data$does_this_tweet_contain_hate_speech))
raw_data$class <- as.numeric(raw_data$does_this_tweet_contain_hate_speech)
```





```{r bar-plot-using-function-call}
source("004-display-bargraph.R")
plotBarGraph(raw_data)
```

## Data pre-processing

As we see, we have a total of 14,509 tweets. When we tried to run our processing scripts over this data, we ended up generating huge intermdiate dataframes causing our local devices to hang or break down. Therefore for our replication project, we have taken a random sample of 3000 tweets from the original dataset 'twitter-hate-speech-classifier-data.csv'. This selection has been made after several attempts to process the data on a local machine with limited processing power.

```{r}

sampled_data <- read.csv(here("Data","sampled_tweet_dataset.csv"))

sampled_data$class <- as.numeric(sampled_data$does_this_tweet_contain_hate_speech)

```


```{r bar-plot-using-function-call}
source("004-display-bargraph.R")
plotBarGraph(sampled_data)
```

As part of pre-processing, following steps were taken to clean the tweet_text column.

1. Converting all strings to lower case - Used base R (tolower function)
2. Removing the punctuations - Used stringr package 
3. Removing the stopgap words - Used tm package

Here is the quick comparison between raw and cleaned tweets after pre-processing steps.

```{r Loading-Libraries}

```


```{r Data-Preparation}
data.frame(refined_data$tweet_text, refined_data$tweet_clean1)
```

## Tokenization and one-hot encoding

The next step in reproducing is to create features for the model by tokeninzing the tweets. In original python code, the authors have used nltk library. We tried various packages in R which provide tokenization functions. Ultimately, we found that 'quanteda' package in R was most suitable to generate similar tokens as original code. 

The following code chunk provides a glimpse of a tweet before and after tokenization.

```{r tokenization}

#To be fixed Tokenization
#data.frame(refined_data$tweet_text, refined_data$tweet_clean1)

```

After tokenization, the next step is to encode the tokens using one-hot encoding to generate features. After one-hot encoding the output csv file was stored in the data folder as **'prepared_tweet_dataset.csv'**. This contains the one-hot encoded data for the selected 3000 tweets which is an input to run the machine learning models.

This dataset can be used to bypass the resource intensive pre-processing steps. It can be used to directly train the model and reproduce the figures that we have claimed to replicate using this project.


## Modeling assumptions

We have implemented Logistic Regression Classifier out of the 5 different machine learning algorithms used by the authors. We chose logistic regression due to better familiarity and ease of implementation.

Further, we have chosen to replicate only the word-level representation of the original paper. Attempting characeter-level representaion was not feasible given the time and computing power limitations.

## Varying Technical Details

### Software
The original code for the ‘Comparative Studies of Detecting Abusive Language on Twitter’ was written in language Python and also used TensorFlow. For our replication project, we have attempted to reproduce the machine learning model for Logistic Regression in the language R. This involved using different packages and functions in R to process the data as well as build the model and obtain the results.

### Dataset
Additionally, due to limited computational capacity, we randomly selected 3000 tweets from a dataset of 14,509 tweets from the ‘Hate Speech Dataset’ provided by Twitter. In the original research paper, the authors were able to execute the models with a dataset containing 100k tweets approximately. This was not necessary for us as we were not building the complex neural network models which require large amounts of data.

### Model
Of the multiple models, we chose to replicate the ‘Logistic Regression’ model using word-level features. This choice was made as the author stated that this model outperformed all the machine learning techniques and had an F1-score which was equivalent to the best CNN model. For our project, we also had limited computational resources due to which execution of other machine learning and deep learning models was out of scope.
 

## Results

Here is a snapshot of the original figures claimed by the authors.

** add original numbers here **

We are however observing lower precision and recall values than the claimed values. There could be a number of different factors causing this mismatch in the figures. Some of them are the following:

1. We are using considerablly smaller dataset compared to the authors. This was done due to limited processing power.
2. While converting the code repository from Python to R, some of the library functions used in R were not exactly the same as those in Python. These small differences in intermediate stages could have inflated the final diference in figures.
3. Perhaps, the original code itself is not very reproducible and has tendency to generate different results with slight changes.


## References
